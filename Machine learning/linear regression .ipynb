{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860f8c21-ff3c-4111-badc-576b77fd76f8",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c197bbb-8e55-462f-9b42-695b05d6bb09",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and aims to find the best-fit line that minimizes the difference between the predicted and actual values. The goal of linear regression is to make predictions or understand the impact of independent variables on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401270fd-92d9-444b-9eb1-12082d8f71e3",
   "metadata": {},
   "source": [
    "In simple linear regression, we consider a single independent variable and a single dependent variable. The relationship between the variables can be represented by the equation:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the dependent variable\n",
    "- `x` is the independent variable\n",
    "- `c` is the y-intercept (the value of `y` when `x` is 0)\n",
    "- `m` is the slope (the change in `y` for a unit change in `x`)\n",
    "\n",
    "In higher dimension this equation becomes:\n",
    "\n",
    "$y = wx + b$\n",
    "\n",
    "The goal is to estimate the values of $w$ and $b$ that best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43364d2e-014e-45f1-b705-f2fe62213b72",
   "metadata": {},
   "source": [
    "**Data**\n",
    "\n",
    "Input: $x$ (i.e, measurements, covariates, features, independent variables)\n",
    "\n",
    "Output: $y$ (i.e., response, dependent variable)\n",
    "\n",
    "\n",
    "\n",
    "**Goal**\n",
    "\n",
    "You need to find a regression function $y\\approx f(x, \\beta)$, where $\\beta$ is the parameter to be estimated from observations.\n",
    "\n",
    "For Simple Linear regression: $y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "\n",
    "For Multiple Linear regression: $y  = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\cdots + \\beta_{d}x_{d}$, where $d$ is the number of features.\n",
    "\n",
    "\n",
    "A regression method is linear if the prediction $f$ is a linear function of the unknown parameters $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe02aa-4176-4341-9372-3e13d5bd9184",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "\n",
    "https://www.geeksforgeeks.org/assumptions-of-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a348e2d-dbfb-430f-abc2-ea2390955968",
   "metadata": {},
   "source": [
    "- __Linear regression should be linear in parameters__\n",
    "\n",
    "  The response variable, $y$ is the function of input variables, $x$'s and parameters, $\\beta$'s. But the linearity condition is up to parameters. The output variable should be linear in terms of parameters, not necessarily in terms of input variables.\n",
    "\n",
    "  For example: \n",
    "  \n",
    "  The equation below is linear in terms of both inputs and parameters, so hold the assumption.\n",
    "\n",
    "  $$y = \\beta_0 + \\beta_1x$$  \n",
    "\n",
    "  Similarly, equation below is not linear in terms of inputs but linear in terms of parmaeters so it holds the assumption.\n",
    "\n",
    "  $$y = \\beta_0 +\\beta_1x^2$$\n",
    "\n",
    "  Lastly, the equation below is linear in terms of input but is not linear in terms of parameters, so it violates the assumption and is not a linear regression model.\n",
    "\n",
    "  $$y = \\beta_0 +\\beta_1^2x$$\n",
    "\n",
    "- __There shouldn't be multicollinearity.__\n",
    "\n",
    "  Multi colinearity here means perfect colinearity. This assumption is for input variables. In simple linear regression, where we have a single input variable, this assumption doesn't play any role, but in case of multiple linear regression, we should be careful. Any two or more sets of input variables should not be perfectly correlated. Perfect correlation might not make the predictor's matrix full rank, which creates a problem in estimating the parameters.\n",
    "\n",
    "  For example, while predicting the house price, you can have many input variables, _length_, _breadth_, _area_, _location_, and many more. In this case, if you include the feature, _area_  along with _length_, _breadth_, you might violate the assumptions because:\n",
    "\n",
    "  $$\\text{area} = \\text{length}\\times\\text{breadth}$$\n",
    "\n",
    "  In such a situation, it is better to drop one of the three input variables from the linear regression model.\n",
    "\n",
    "- __There should be a random sampling of observations.__\n",
    "\n",
    "  The observations for the linear regression should be randomly sampled from any population. Suppose, you are trying to build a regression model to know the factors that affect the price of the house, then you must select houses randomly from a locality, rather than adopting a convenient sampling procedure. Also, the number of observations should always be higher than the number of parameters to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a1a9c-336f-4618-b8b9-5781afc617ac",
   "metadata": {},
   "source": [
    "# Parameter Estimation Techniques\n",
    "\n",
    "Now that you know the assumptions of Linear Regression, it's time you know the techniques to find the parameters, i.e., intercept and regression coefficients. Two of them are as follows:\n",
    "\n",
    "1. Least Squares Estimation\n",
    "\n",
    "  In practice, observed data or input-output pair is given, and $\\beta's$ are unknown. We use the given input-output pair to estimate the coefficients. The estimated intercept and regression coefficient later helps us in predicting the output value with the input values. There is an error or residual since the estimated regression can not satisfy all the output data points. Hence, we have an error or residual, which is the difference between the estimated output value and the actual output value. Errors can either be positive or negative. We can sum the errors to evaluate the estimated linear regression line. To get rid of the cancellation of the positive and negative error, we square the error and add then which is popularly called as Sum of Squares of errors or Residual Sum of Squares.\n",
    "\n",
    "  This parameter estimation technique finds the parameters by minimizing the sum of squares. There are mainly three types of Least Squares:\n",
    "\n",
    "      - Ordinary Least Squares\n",
    "\n",
    "      - Weighted Least Squares\n",
    "\n",
    "      - Generalized Least Squares\n",
    "\n",
    "\n",
    "2. Maximum Likelihood Estimation\n",
    "\n",
    "    Maximum likelihood estimation is a well known probabilistic framework for finding parameters that best describe the observed data. You find the parameters by maximizing the conditional probability of observing the data, $x$ given specific probability distribution and its parameters $\\beta's$. Detailed discussion on this technique is out of the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17277dc5-1497-4ce9-8ce2-fe7085ede500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
