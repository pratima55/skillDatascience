{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e86f7ec-2315-40ff-b024-d6e80990ace4",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares with Simple Linear Regression\n",
    "\n",
    "\n",
    "The simple linear regression model is:\n",
    "\n",
    " $$\\mathbf{y} = \\beta_0 +\\beta_1\\mathbf{x}$$\n",
    "\n",
    "\n",
    "where, we need to estimate the parameters, intercept($\\beta_0$) and slope($\\beta_1$). \n",
    "\n",
    "\n",
    "Let's recall an Advertising dataset and simple linear regression performed on scatter plot of _sales_ Vs. _TV_. With the help of `Scikit-Learn`, we were able to fit the best regression line among all the possibilities. Here is a snapshot:\n",
    "\n",
    "<figure align=\"center\">\n",
    "       <img src=\"fig1.png\" height=\"350\" width=\"600\">\n",
    "       <figcaption>Figure 1: Simple Linear Regression </figcaption>\n",
    "   </figure>\n",
    "\n",
    "\n",
    "\n",
    "The blue line is a simple linear regression line with output $\\mathbf{y}$ as `sales` and $\\mathbf{x}$ as `TV`. The residual or error, $\\epsilon$ is the difference between the observed value, $y_i$, and predicted value, $\\hat{y_i}$. The observed value is the actual output data point, which is all blue dots in the figure, and the predicted value is the point given by the black regression line. Error for each output data point is shown by the vertical distance from the actual output data point to the predicted point on a regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9ef92-979c-4755-ba4b-fbd35c1d93af",
   "metadata": {},
   "source": [
    "The predicted output value is:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0 + \\beta_1x_i$$\n",
    "\n",
    "The observed (actual) output value is:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$$\n",
    "\n",
    "Where $\\epsilon_i$ is a random error, not a parameter. The error $\\epsilon_i$ as ($y_{i}-\\hat{y_{i}}$) can either be positive or negative or even 0 sometimes. As we can see in the figure, vertical lines are on either side of the regression line. To avoid the cancellation of the error while summing errors, we square each error and sum them, called _Residual Sum of Squares (RSS)_ or _Sum of Squared Errors (SSE)_.\n",
    "\n",
    "$$\\text{Sum of Squared Errors (SSE)} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2$$\n",
    "\n",
    "The summation is indexed from $1$ to $n$, since we have $n$ samples. Sum of Squared Errors (SSE) is the function of $\\beta_0$ and $\\beta_1$. We can also take it as _Loss function_. The main principle of Least Squares is that we should end up choosing intercept ($\\beta_0$) and slope ($\\beta_1$) such that the overall sum is minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd50e9-0e35-4f1f-9cc0-edaf2bed254f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
